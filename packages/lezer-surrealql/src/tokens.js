import {ExternalTokenizer} from "@lezer/lr";

import {
	analyzer,
	any,
	as,
	asc,
	assert,
	at,
	begin,
	bm25,
	_break,
	by,
	cancel,
	capacity,
	changefeed,
	changes,
	columns,
	comment,
	commit,
	content,
	_continue,
	create,
	database,
	db,
	_default,
	define,
	_delete,
	desc,
	dimension,
	dist,
	doc_ids_cache,
	doc_ids_order,
	doc_lengths_cache,
	doc_lengths_order,
	drop,
	duplicate,
	efc,
	_else,
	end,
	explain,
	exists,
	extend_candidates,
	event,
	fetch,
	field,
	fields,
	flexible,
	_for,
	from,
	group,
	mtree,
	mtree_cache,
	highlights,
	hnsw,
	_if,
	ignore,
	_in,
	index,
	info,
	insert,
	into,
	keep_pruned_connections,
	key,
	kill,
	_let,
	limit,
	live,
	lm,
	m,
	m0,
	merge,
	namespace,
	noindex,
	normal,
	not,
	ns,
	on,
	only,
	option,
	order,
	out,
	parallel,
	param,
	passhash,
	password,
	patch,
	permissions,
	postings_cache,
	postings_order,
	readonly,
	rebuild,
	relate,
	relation,
	remove,
	_return,
	roles,
	root,
	sc,
	scope,
	schemafull,
	schemaless,
	search,
	select,
	session,
	set,
	show,
	since,
	signin,
	signup,
	sleep,
	split,
	start,
	structure,
	tb,
	table,
	terms_cache,
	terms_order,
	then,
	_throw,
	timeout,
	to,
	token,
	tokenizers,
	transaction,
	type,
	unique,
	unset,
	update,
	use,
	user,
	value,
	values,
	when,
	where,
	_with,

	// Literals
	after,
	before,
	diff,
	_false,
	full,
	none,
	_null,
	_true,

	f32,
	f64,
	i16,
	i32,
	i64,

	updatePermissions,
	createPermissions,
	deletePermissions,
	selectPermissions,

	jwks,
	eddsa,
	es256,
	es384,
	es512,
	ps256,
	ps384,
	ps512,
	rs256,
	rs384,
	rs512,

	and,
	or,
	is,
	opNot,
	opIn,
	contains,
	containsnot,
	containsall,
	containsany,
	containsnone,
	inside,
	notinside,
	allinside,
	anyinside,
	noneinside,
	outside,
	intersects,

	chebyshev,
	cosine,
	euclidean,
	hamming,
	jaccard,
	manhattan,
	minkowski,
	pearson,

	_function,
	rand,
	count,

	objectOpen,
} from "./parser.terms";

const tokenMap = {
	analyzer,
	any,
	as,
	asc,
	assert,
	at,
	begin,
	bm25,
	break: _break,
	by,
	cancel,
	capacity,
	changefeed,
	changes,
	columns,
	comment,
	commit,
	content,
	continue: _continue,
	create,
	database,
	db,
	default: _default,
	define,
	delete: _delete,
	desc,
	dimension,
	dist,
	doc_ids_cache,
	doc_ids_order,
	doc_lengths_cache,
	doc_lengths_order,
	drop,
	duplicate,
	efc,
	else: _else,
	end,
	exists,
	explain,
	extend_candidates,
	event,
	fetch,
	field,
	fields,
	flexible,
	for: _for,
	from,
	group,
	highlights,
	hnsw,
	if: _if,
	ignore,
	in: _in,
	index,
	info,
	insert,
	into,
	keep_pruned_connections,
	key,
	kill,
	let: _let,
	limit,
	live,
	lm,
	m,
	m0,
	merge,
	mtree,
	mtree_cache,
	namespace,
	noindex,
	normal,
	not,
	ns,
	on,
	only,
	option,
	order,
	out,
	parallel,
	param,
	passhash,
	password,
	patch,
	permissions,
	postings_cache,
	postings_order,
	readonly,
	rebuild,
	relate,
	relation,
	remove,
	return: _return,
	roles,
	root,
	sc,
	scope,
	schemafull,
	schemaless,
	search,
	select,
	session,
	set,
	show,
	since,
	signin,
	signup,
	sleep,
	split,
	start,
	structure,
	tb,
	table,
	terms_cache,
	terms_order,
	then,
	throw: _throw,
	timeout,
	to,
	token,
	tokenizers,
	transaction,
	type,
	unique,
	unset,
	update,
	use,
	user,
	value,
	values,
	when,
	where,
	with: _with,

	// Literals
	after,
	before,
	diff,
	false: _false,
	full,
	none,
	null: _null,
	true: _true,

	f32,
	f64,
	i16,
	i32,
	i64,

	jwks,
	eddsa,
	es256,
	es384,
	es512,
	ps256,
	ps384,
	ps512,
	rs256,
	rs384,
	rs512,

	and,
	or,
	is,
	contains,
	containsnot,
	containsall,
	containsany,
	containsnone,
	inside,
	notinside,
	allinside,
	anyinside,
	noneinside,
	outside,
	intersects,

	chebyshev,
	cosine,
	euclidean,
	hamming,
	jaccard,
	manhattan,
	minkowski,
	pearson,

	// Function names
	function: _function,
	rand,
	count,
};

const tryMapped = {
	select: [selectPermissions],
	create: [createPermissions],
	update: [updatePermissions],
	delete: [deletePermissions],
	not: [opNot],
	in: [opIn],
};

export const tokens = function(t, stack) {
	for (const tk of tryMapped[t.toLowerCase()] ?? []) {
		if (stack.canShift(tk)) return tk;
	}

	return tokenMap[t.toLowerCase()] ?? -1;
}

function skipSpace(input, off) {
	for (;;) {
		let next = input.peek(off);
		if (next === 32 || next === 9 || next === 10 || next === 13) {
			off++;
		} else if (next === 35 /* '#' */ ||
				   (next === 47 /* '/' */ || next === 45 /* '-' */) && input.peek(off + 1) === next) {
			off++;
			for (;;) {
				let next = input.peek(off);
				if (next < 0 || next === 10 || next === 13) break;
				off++;
			}
		} else {
			return off;
		}
	}
}

function isIdentifierChar(ch) {
	return ch === 95 || ch >= 65 && ch <= 90 || ch >= 97 && ch <= 122 || ch >= 48 && ch <= 57;
}

function skipObjKey(input, off) {
	let first = input.peek(off);
	if (isIdentifierChar(first)) {
		do {
			off++;
		} while (isIdentifierChar(input.peek(off)));
		return off;
	} else if (first === 38 /* "'" */ || first === 34 /* '"' */) {
		for (;;) {
			let next = input.peek(++off);
			if (next < 0) return off;
			if (next === first) return off + 1;
		}
	}
}

export const objectToken = new ExternalTokenizer((input, stack) => {
	if (input.next === 123 /* '{' */) {
		let off = skipSpace(input, 1);
		let key = skipObjKey(input, off);
		if (key !== null) {
			off = skipSpace(input, key);
			if (input.peek(off) === 58 /* ':' */) {
				input.acceptToken(objectOpen, 1);
			}
		}
	}
});
